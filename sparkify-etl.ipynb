{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Prototype, Development, Testing of Sparkify ETL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, row_number\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Handle Credentials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']= config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']= config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Spark Session**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f7b78661ef0>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\",\"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.awsAccessKeyId\", os.environ['AWS_ACCESS_KEY_ID']) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.awsSecretAccessKey\", os.environ['AWS_SECRET_ACCESS_KEY']) \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", config['AWS']['AWS_ACCESS_KEY_ID'])\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", config['AWS']['AWS_SECRET_ACCESS_KEY'])\n",
    "\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Read Song data file**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: string (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#song_data = 'data/song-data/*/*/*/*.json'\n",
    "song_data = 's3a://udacity-dend/song_data/A/B/C/TRABCEI128F424C983.json'\n",
    "df_song = spark.read.json(song_data)\n",
    "df_song.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Create song table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      "\n",
      "+------------------+-------------------+------------------+----+---------+\n",
      "|           song_id|              title|         artist_id|year| duration|\n",
      "+------------------+-------------------+------------------+----+---------+\n",
      "|SOUPIRU12A6D4FA1E1|Der Kleine Dompfaff|ARJIE2Y1187B994AB7|   0|152.92036|\n",
      "+------------------+-------------------+------------------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_song.createOrReplaceTempView(\"staging_songs_table\")\n",
    "songs_table = spark.sql(\"\"\"SELECT song_id, title, artist_id, year, duration FROM staging_songs_table ORDER BY song_id\"\"\").dropDuplicates()\n",
    "songs_table.printSchema()\n",
    "songs_table.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Test write song table to partitioned parquet files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reference: https://knowledge.udacity.com/questions/103172\n",
    "songsParquetPath = \"{}{}\".format('data/output-data/','songs.parquet')\n",
    "songs_table.write.mode('overwrite').partitionBy(\"year\",\"artist_id\").parquet(songsParquetPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Create Artists Table Create and write artists table to parquet file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- lattitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      "\n",
      "+------------------+-----------+--------+---------+---------+\n",
      "|         artist_id|       name|location|lattitude|longitude|\n",
      "+------------------+-----------+--------+---------+---------+\n",
      "|ARJIE2Y1187B994AB7|Line Renaud|        |     null|     null|\n",
      "+------------------+-----------+--------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artists_table = spark.sql(\"\"\" SELECT artist_id, artist_name AS name, artist_location AS location, \n",
    "                                     artist_latitude AS lattitude, artist_longitude AS longitude \n",
    "                              FROM staging_songs_table \n",
    "                              ORDER BY artist_id \"\"\").dropDuplicates()\n",
    "artists_table.printSchema()\n",
    "artists_table.show(5)\n",
    "\n",
    "artistsParquetPath = \"{}{}\".format('data/output-data/','artists.parquet')\n",
    "artists_table.write.mode('overwrite').parquet(artistsParquetPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Read Log data (picking one sample file from log data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_data = 's3a://udacity-dend/log_data/*/*/*.json'\n",
    "#log_data =  'data/log-data/*.json'\n",
    "df_log = spark.read.json(log_data)\n",
    "df_log.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Log data staging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+---------+------+-------------+---------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|            artist|     auth|firstName|gender|itemInSession| lastName|   length|level|            location|method|    page|     registration|sessionId|                song|status|           ts|           userAgent|userId|\n",
      "+------------------+---------+---------+------+-------------+---------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "|           Fat Joe|Logged In|     Kate|     F|           21|  Harrell|241.34485| paid|Lansing-East Lans...|   PUT|NextSong|1.540472624796E12|      605|Safe 2 Say [The I...|   200|1542296032796|\"Mozilla/5.0 (X11...|    97|\n",
      "|       Linkin Park|Logged In|     Kate|     F|           33|  Harrell|259.86567| paid|Lansing-East Lans...|   PUT|NextSong|1.540472624796E12|      605|         My December|   200|1542299023796|\"Mozilla/5.0 (X11...|    97|\n",
      "|     The Saturdays|Logged In|    Chloe|     F|           20|   Cuevas|176.95302| paid|San Francisco-Oak...|   PUT|NextSong|1.540940782796E12|      630|     If This Is Love|   200|1542318319796|Mozilla/5.0 (Wind...|    49|\n",
      "|       Wim Mertens|Logged In|   Aleena|     F|           71|    Kirby|240.79628| paid|Waterloo-Cedar Fa...|   PUT|NextSong|1.541022995796E12|      619|          Naviamente|   200|1542321121796|Mozilla/5.0 (Maci...|    44|\n",
      "|The Avett Brothers|Logged In| Mohammad|     M|            1|Rodriguez| 271.0722| paid|Sacramento--Rosev...|   PUT|NextSong|1.540511766796E12|      744|   The Perfect Space|   200|1542786093796|\"Mozilla/5.0 (Mac...|    88|\n",
      "+------------------+---------+---------+------+-------------+---------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_log.createOrReplaceTempView(\"staging_log_data\")\n",
    "log_table = spark.sql(\"\"\"SELECT * FROM staging_log_data WHERE page='NextSong'\"\"\").dropDuplicates()\n",
    "log_table.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Create Users table and write parquet file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      "\n",
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "|       |      null|     null|  null| paid|\n",
      "|       |      null|     null|  null| free|\n",
      "|     10|    Sylvie|     Cruz|     F| free|\n",
      "|    100|     Adler|  Barrera|     M| free|\n",
      "|    101|    Jayden|      Fox|     M| free|\n",
      "+-------+----------+---------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_table = spark.sql(\"\"\" SELECT userId AS user_id, firstName AS first_name, lastName AS last_name, gender, level \n",
    "                            FROM staging_log_data \n",
    "                            ORDER BY user_id \"\"\").dropDuplicates()\n",
    "users_table.printSchema()\n",
    "users_table.show(5)\n",
    "\n",
    "usersParquetPath = \"{}{}\".format('data/output-data/','users.parquet')\n",
    "users_table.write.mode('overwrite').parquet(usersParquetPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**create timestamp column from original timestamp column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+-------------------+-------------------+\n",
      "| artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page|     registration|sessionId|                song|status|           ts|           userAgent|userId|          timestamp|           datetime|\n",
      "+-------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+-------------------+-------------------+\n",
      "|Fat Joe|Logged In|     Kate|     F|           21| Harrell|241.34485| paid|Lansing-East Lans...|   PUT|NextSong|1.540472624796E12|      605|Safe 2 Say [The I...|   200|1542296032796|\"Mozilla/5.0 (X11...|    97|2018-11-15 15:33:52|2018-11-15 15:33:52|\n",
      "+-------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-----------------+---------+--------------------+------+-------------+--------------------+------+-------------------+-------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "root\n",
      " |-- start_time: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      "\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|         start_time|hour|day|week|month|year|weekday|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|2018-11-01 21:01:46|  21|  1|  44|   11|2018|      5|\n",
      "|2018-11-01 21:05:52|  21|  1|  44|   11|2018|      5|\n",
      "|2018-11-01 21:08:16|  21|  1|  44|   11|2018|      5|\n",
      "|2018-11-01 21:11:13|  21|  1|  44|   11|2018|      5|\n",
      "|2018-11-01 21:17:33|  21|  1|  44|   11|2018|      5|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Refernce: https://knowledge.udacity.com/questions/67777\n",
    "\n",
    "# create timestamp column from original timestamp column\n",
    "get_timestamp = udf(lambda x: datetime.fromtimestamp(x/1000.0).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "log_table = log_table.withColumn('timestamp', get_timestamp('ts'))\n",
    " \n",
    "# create datetime column from original timestamp column\n",
    "get_datetime = udf(lambda x: datetime.fromtimestamp(x/1000.0).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "log_table = log_table.withColumn('datetime', get_datetime('ts'))\n",
    "\n",
    "# see if timestamp and datetime have values as expected.\n",
    "log_table.show(1)\n",
    "\n",
    "# extract columns (start_time, hour, day, week, month, year, weekday) to create time table\n",
    "log_table.createOrReplaceTempView(\"staging_time\")\n",
    "time_table = spark.sql(\"\"\"\n",
    "    SELECT datetime AS start_time, hour(timestamp) AS hour, day(timestamp) AS day, \n",
    "           weekofyear(timestamp) AS week, month(timestamp) AS month, year(timestamp) AS year,\n",
    "           dayofweek(timestamp) AS weekday\n",
    "    FROM staging_time\n",
    "    ORDER BY start_time\n",
    "\"\"\").dropDuplicates()\n",
    "\n",
    "time_table.printSchema()\n",
    "time_table.show(5)\n",
    " \n",
    "# write time table to parquet files partitioned by year and month\n",
    "timesParquetPath = \"{}{}\".format('data/output-data/','times.parquet')\n",
    "time_table.write.mode('overwrite').partitionBy(\"year\", \"month\").parquet(timesParquetPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Create songplays table and write parquet file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+-------+---------+----------+--------+----------+----+-----+-----------+\n",
      "|start_time|user_id|level|song_id|artist_id|session_id|location|user_agent|year|month|songplay_id|\n",
      "+----------+-------+-----+-------+---------+----------+--------+----------+----+-----+-----------+\n",
      "+----------+-------+-----+-------+---------+----------+--------+----------+----+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://knowledge.udacity.com/questions/150979\n",
    "# Refernce: https://stackoverflow.com/questions/40508489/spark-merge-2-dataframes-by-adding-row-index-number-on-both-dataframes\n",
    "\n",
    "# read in song data to use for songplays table\n",
    "songs_table = spark.read.parquet('data/output-data/songs.parquet')\n",
    "artist_table = spark.read.parquet('data/output-data/artists.parquet')\n",
    "\n",
    "# extract columns from joined song and log datasets to create songplays table \n",
    "#songplays: songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent\n",
    "w = Window().orderBy('song_id')\n",
    "df_joined = log_table.join(songs_table.alias(\"st\"),   log_table.song   == col('st.title')) \\\n",
    "                     .join(artist_table.alias(\"at\"),  log_table.artist == col('at.name')) \\\n",
    "                     .select(\n",
    "                         col('timestamp').alias('start_time'),\n",
    "                         col('userId').alias('user_id'),\n",
    "                         'level',\n",
    "                         'st.song_id',\n",
    "                         'at.artist_id',\n",
    "                         col('sessionId').alias('session_id'),\n",
    "                         'at.location',\n",
    "                         col('userAgent').alias('user_agent'),\n",
    "                         year(col('timestamp')).alias('year'), \n",
    "                         month(col('timestamp')).alias('month')) \\\n",
    "                     .withColumn('songplay_id', row_number().over(w))\n",
    "\n",
    "df_joined.createOrReplaceTempView(\"songplays_staging\")\n",
    "songplays_table = spark.sql(\"\"\"SELECT * FROM songplays_staging\"\"\").dropDuplicates()\n",
    "songplays_table.show(5)\n",
    "\n",
    "# write songplays table to parquet files partitioned by year and month\n",
    "# Reference: https://stackoverflow.com/questions/50962934/partition-column-is-moved-to-end-of-row-when-saving-a-file-to-parquet\n",
    "# According to the above reference parition field year and month are not written into the parquet file and only as folder names..year and month\n",
    "\n",
    "#songplays_table\n",
    "songplaysParquetPath = \"{}{}\".format('data/output-data/','songplays.parquet')\n",
    "songplays_table.write.mode('overwrite').partitionBy(\"year\",\"month\").parquet(songplaysParquetPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Alternative Way - if not using parquet files.\n",
    "#df_joined = df_log.join(df_song, (df_log.song   == df_song.title) & \n",
    "#                                 (df_log.artist == df_song.artist_name) &  \n",
    "#                                 (df_log.length == df_song.duration), 'left_outer').select(\n",
    "#                                    df_log.ts.alias('start_time'),\n",
    "#                                    col('userId').alias('user_id'),\n",
    "#                                    df_log.level,\n",
    "#                                    col('song_id'),\n",
    "#                                    col('artist_id'),\n",
    "#                                    df_log.sessionId.alias('session_id'),\n",
    "#                                    col('artist_location').alias('location'),\n",
    "#                                    col('userAgent').alias('user_agent')\n",
    "#                                 ).withColumn('songplay_id', row_number().over(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
